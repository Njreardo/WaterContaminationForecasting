{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "upset-daily",
   "metadata": {},
   "source": [
    "## Water Quality Projections in Los Angeles\n",
    "### Nick Reardon\n",
    "### 2/14/2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "subtle-tomato",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "\n",
    "# Make numpy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "pd.set_option('max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "departmental-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./LA water quality.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "military-miami",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2356, 11)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "royal-testing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Policy Area', 'Dataset', 'Variable', 'Year', 'Contaminant Count',\n",
       "       'Tract', 'Tract Number', 'Neighborhood', 'GEOID', 'Row ID', 'Date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hungry-guinea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform text into features\n",
    "\n",
    "ml_df = pd.get_dummies(df, 'category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "plastic-scheduling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2356, 7310)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fewer-writer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Get our datasets and impute missing values\n",
    "X = ml_df.copy()\n",
    "y = pd.DataFrame(KNNImputer().fit_transform(X), columns=ml_df.columns)['Contaminant Count']\n",
    "del X['Contaminant Count']\n",
    "\n",
    "# Standardize X\n",
    "X = pd.DataFrame(StandardScaler().fit_transform(X), columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adverse-mouse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2356,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "mighty-attraction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    570.923693\n",
       "1    555.052464\n",
       "2    436.468459\n",
       "3    436.468459\n",
       "4    581.035658\n",
       "Name: Contaminant Count, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "decimal-analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'> (2356, 7309)\n"
     ]
    }
   ],
   "source": [
    "print(type(X), X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "exciting-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "worth-basement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "et = ExtraTreesRegressor(n_estimators=1000)\n",
    "model = et.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "worse-austin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7194761497303439"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Previous run: R2=.718\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "naughty-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Percentage Error\n",
    "\n",
    "def mape(actual, pred): \n",
    "    actual, pred = np.array(actual), np.array(pred)\n",
    "    return np.mean(np.abs((actual - pred) / actual)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "amber-filename",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "equivalent-cardiff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.097344796050072"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-kazakhstan",
   "metadata": {},
   "source": [
    "#### A MAPE that low is a very good sign. Our model is performing well. Let's see if NN's can accomplish something better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "indirect-robinson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "forward-provision",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(units=X.shape[-1], activation='relu'),\n",
    "    keras.layers.Dense(units=X.shape[-1]*1.5, activation='relu'),\n",
    "    keras.layers.Dense(units=X.shape[-1]*1.5, activation='relu'),\n",
    "    keras.layers.Dense(units=1, activation='relu')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "indian-mitchell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1578 samples, validate on 778 samples\n",
      "Epoch 1/15\n",
      "1578/1578 [==============================] - 78s 49ms/sample - loss: 3581.5256 - mae: 42.7223 - val_loss: 56197.0090 - val_mae: 486.2235\n",
      "Epoch 2/15\n",
      "1578/1578 [==============================] - 77s 49ms/sample - loss: 2668.4744 - mae: 43.7923 - val_loss: 57787.9904 - val_mae: 493.5894\n",
      "Epoch 3/15\n",
      "1578/1578 [==============================] - 79s 50ms/sample - loss: 2237.7661 - mae: 36.5061 - val_loss: 56407.0784 - val_mae: 487.1803\n",
      "Epoch 4/15\n",
      "1578/1578 [==============================] - 81s 51ms/sample - loss: 2155.1454 - mae: 36.0727 - val_loss: 57424.0013 - val_mae: 491.7710\n",
      "Epoch 5/15\n",
      "1578/1578 [==============================] - 79s 50ms/sample - loss: 2477.9914 - mae: 38.2723 - val_loss: 57254.0938 - val_mae: 491.2667\n",
      "Epoch 6/15\n",
      "1578/1578 [==============================] - 77s 49ms/sample - loss: 1881.5581 - mae: 33.8211 - val_loss: 61249.5900 - val_mae: 509.9758\n",
      "Epoch 7/15\n",
      "1578/1578 [==============================] - 77s 49ms/sample - loss: 1305.3986 - mae: 27.6536 - val_loss: 57941.3728 - val_mae: 494.3849\n",
      "Epoch 8/15\n",
      "1578/1578 [==============================] - 79s 50ms/sample - loss: 1358.1353 - mae: 28.5089 - val_loss: 58087.8413 - val_mae: 494.9588\n",
      "Epoch 9/15\n",
      "1578/1578 [==============================] - 77s 49ms/sample - loss: 955.1242 - mae: 23.7020 - val_loss: 59115.9216 - val_mae: 499.8521\n",
      "Epoch 10/15\n",
      "1578/1578 [==============================] - 78s 49ms/sample - loss: 857.5966 - mae: 22.6539 - val_loss: 56262.8104 - val_mae: 486.0331\n",
      "Epoch 11/15\n",
      "1578/1578 [==============================] - 77s 49ms/sample - loss: 557.1107 - mae: 18.1010 - val_loss: 57942.7095 - val_mae: 494.1468\n",
      "Epoch 12/15\n",
      "1578/1578 [==============================] - 78s 49ms/sample - loss: 455.9933 - mae: 16.1612 - val_loss: 56820.5006 - val_mae: 488.6758\n",
      "Epoch 13/15\n",
      "1578/1578 [==============================] - 78s 49ms/sample - loss: 397.3439 - mae: 15.5631 - val_loss: 57315.0193 - val_mae: 491.0602\n",
      "Epoch 14/15\n",
      "1578/1578 [==============================] - 78s 50ms/sample - loss: 488.2604 - mae: 16.9803 - val_loss: 58249.7082 - val_mae: 495.5903\n",
      "Epoch 15/15\n",
      "1578/1578 [==============================] - 77s 49ms/sample - loss: 338.5354 - mae: 13.9430 - val_loss: 57070.1922 - val_mae: 489.7916\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mae'])\n",
    "\n",
    "history = model.fit(\n",
    "                    X_train.values,\n",
    "                    y_train.values,\n",
    "                    epochs=15,\n",
    "                    validation_data=(X_test.values, y_test.values),\n",
    "                    validation_steps=5\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "contrary-vienna",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_predictions = model.predict(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "quarterly-combine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84.98307206964917"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see the MAPE for our neural network now\n",
    "\n",
    "mape(y_test, nn_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-policy",
   "metadata": {},
   "source": [
    "#### In this case we most likely do not want to go with this neural network. ExtraTree's works fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-exhaust",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
